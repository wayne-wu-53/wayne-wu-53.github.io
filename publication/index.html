<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="googlea477729bc8866235"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publication | Wayne (Wen-Yan) Wu</title> <meta name="author" content="Wayne (Wen-Yan) Wu"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/milky-way.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wayne-wu-53.github.io/publication/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://wayne-wu-53.github.io/">Wayne (Wen-Yan) Wu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publication/">Publication<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Project</a> </li> <li class="nav-item "> <a class="nav-link" href="/dataset/">Dataset</a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">Software</a> </li> <li class="nav-item "> <a class="nav-link" href="/Others/">Others</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publication</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/contributors2023renderme360-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/contributors2023renderme360-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/contributors2023renderme360-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/contributors2023renderme360.png"> </picture> </figure> </div> <div id="contributors2023renderme360" class="col-sm-9"> <div class="title">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</div> <div class="author"> Contributors to RenderMe-360 </div> <div class="periodical"> <em>coming soon,</em> 2023 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=nIgrtQwkrdg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://renderme-360.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/contributors2023dnarendering-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/contributors2023dnarendering-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/contributors2023dnarendering-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/contributors2023dnarendering.png"> </picture> </figure> </div> <div id="contributors2023dnarendering" class="col-sm-9"> <div class="title">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</div> <div class="author"> Contributors to DNA-Rendering </div> <div class="periodical"> <em>coming soon,</em> 2023 </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">YouTube</a> <a href="" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/honglin2023orthoplanes-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/honglin2023orthoplanes-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/honglin2023orthoplanes-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/honglin2023orthoplanes.png"> </picture> </figure> </div> <div id="honglin2023orthoplanes" class="col-sm-9"> <div class="title">OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs</div> <div class="author">Honglin He, Zhuoqian Yang, Shikai Li, Bo Dai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>coming soon,</em> 2023 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=hHts2zWEbJ8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://orthoplanes.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jianglin2023unitedhuman-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jianglin2023unitedhuman-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jianglin2023unitedhuman-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jianglin2023unitedhuman.png"> </picture> </figure> </div> <div id="jianglin2023unitedhuman" class="col-sm-9"> <div class="title">UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation</div> <div class="author">Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Ziwei Liu, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>coming soon,</em> 2023 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=pdsfUYFDLSw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://unitedhuman.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhitao2023synbody-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhitao2023synbody-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhitao2023synbody-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhitao2023synbody.png"> </picture> </figure> </div> <div id="zhitao2023synbody" class="col-sm-9"> <div class="title">SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling</div> <div class="author">Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai,  <em>Wayne Wu</em>, Chen Qian, Dahua Lin, Ziwei Liu, and Lei Yang </div> <div class="periodical"> <em>Technical report, arXiv:2303.17368,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2303.17368" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=ogXpRB9zR9A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://maoxie.github.io/SynBody/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yuming2023text2performer-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yuming2023text2performer-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yuming2023text2performer-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yuming2023text2performer.png"> </picture> </figure> </div> <div id="yuming2023text2performer" class="col-sm-9"> <div class="title">Text2Performer: Text-Driven Human Video Generation</div> <div class="author">Yuming Jiang, Shuai Yang, Tong Liang Koh,  <em>Wayne Wu</em>, Chen Change Loy, and Ziwei Liu </div> <div class="periodical"> <em>Technical report, arXiv:2304.08483,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2304.08483" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=YwhaJUk_qo0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yumingj.github.io/projects/Text2Performer.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhuo2023hyperstyle3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhuo2023hyperstyle3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhuo2023hyperstyle3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhuo2023hyperstyle3d.png"> </picture> </figure> </div> <div id="zhuo2023hyperstyle3d" class="col-sm-9"> <div class="title">HyperStyle3D: Text-Guided 3D Portrait Stylization via Hypernetworks</div> <div class="author">Zhuo Chen, Xudong Xu,  et al.,  <em>Wayne Wu</em>, Bo Dai, and Xiaokang Yang </div> <div class="periodical"> <em>Technical report, arXiv:2304.09463,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2304.09463" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhengming2023monohuman-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhengming2023monohuman-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhengming2023monohuman-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhengming2023monohuman.png"> </picture> </figure> </div> <div id="zhengming2023monohuman" class="col-sm-9"> <div class="title">MonoHuman: Animatable Human Neural Field from Monocular Video</div> <div class="author">Zhengming Yu, Wei Cheng, Xian Liu,  <em>Wayne Wu</em>, and Kwan-Yee Lin </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2304.02001" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://yzmblog.github.io/projects/MonoHuman/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jianhui2023celebvtext-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jianhui2023celebvtext-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jianhui2023celebvtext-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jianhui2023celebvtext.png"> </picture> </figure> </div> <div id="jianhui2023celebvtext" class="col-sm-9"> <div class="title">CelebV-Text: A Large-Scale Facial Text-Video Dataset</div> <div class="author">Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2303.14717" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://celebv-text.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/tong2023omniobject3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/tong2023omniobject3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/tong2023omniobject3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/tong2023omniobject3d.png"> </picture> </figure> </div> <div id="tong2023omniobject3d" class="col-sm-9"> <div class="title">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation</div> <div class="author">Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan,  <em>Wayne Wu</em>, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2301.07525" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://omniobject3d.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/haoyue2023filter-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/haoyue2023filter-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/haoyue2023filter-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/haoyue2023filter.png"> </picture> </figure> </div> <div id="haoyue2023filter" class="col-sm-9"> <div class="title">Filter-Recovery Network for Multi-Speaker Audio-Visual Speech Separation</div> <div class="author">Haoyue Cheng, Zhaoyang Liu,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR),</em> 2023 </div> <div class="links"> <a href="https://openreview.net/forum?id=fiB2RjmgwQ6" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wei2022gnr-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wei2022gnr-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wei2022gnr-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wei2022gnr.png"> </picture> </figure> </div> <div id="wei2022gnr" class="col-sm-9"> <div class="title">Generalizable Neural Performer: Learning Robust Radiance Fields for Human Novel View Synthesis</div> <div class="author">Wei Cheng, Su Xu, Jingtan Piao, Chen Qian,  <em>Wayne Wu</em>, Kwan-Yee Lin, and Hongsheng Li </div> <div class="periodical"> <em>Technical report, arXiv:2204.11798,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11798" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=2COR4u1ZIuk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://generalizable-neural-performer.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhuoqian2022hg3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhuoqian2022hg3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhuoqian2022hg3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhuoqian2022hg3d.png"> </picture> </figure> </div> <div id="zhuoqian2022hg3d" class="col-sm-9"> <div class="title">3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation</div> <div class="author">Zhuoqian Yang, Shikai Li,  <em>Wayne Wu</em>†, and Bo Dai </div> <div class="periodical"> <em>Technical report, arXiv:2210.06551,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.07378" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=fYgVYGMLpnw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://3dhumangan.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wentao2022motionbert-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wentao2022motionbert-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wentao2022motionbert-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wentao2022motionbert.png"> </picture> </figure> </div> <div id="wentao2022motionbert" class="col-sm-9"> <div class="title">MotionBERT: Unified Pretraining for Human Motion Analysis</div> <div class="author">Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu,  <em>Wayne Wu</em>, and Yizhou Wang </div> <div class="periodical"> <em>Technical report, arXiv:2210.06551,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2210.06551" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://motionbert.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jintao2022vlg-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jintao2022vlg-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jintao2022vlg-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jintao2022vlg.png"> </picture> </figure> </div> <div id="jintao2022vlg" class="col-sm-9"> <div class="title">VLG: General Video Recognition with Web Textual Knowledge</div> <div class="author">Jintao Lin, Zhaoyang Liu, Wenhai Wang,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>Technical report, arXiv:2212.01638,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.01638" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/haonan2022stylefacev-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/haonan2022stylefacev-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/haonan2022stylefacev-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/haonan2022stylefacev.png"> </picture> </figure> </div> <div id="haonan2022stylefacev" class="col-sm-9"> <div class="title">StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3</div> <div class="author">Haonan Qiu, Yuming Jiang, Hang Zhou,  <em>Wayne Wu</em>, and Ziwei Liu </div> <div class="periodical"> <em>Technical report, arXiv:2208.07862,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2208.07862" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=BZNLcD04-Fc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="http://haonanqiu.com/projects/StyleFaceV.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xian2022audio-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xian2022audio-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xian2022audio-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xian2022audio.png"> </picture> </figure> </div> <div id="xian2022audio" class="col-sm-9"> <div class="title">Audio-Driven Co-Speech Gesture Video Generation</div> <div class="author">Xian Liu, Qianyi Wu, Hang Zhou, Yuanqi Du,  <em>Wayne Wu</em>, Dahua Lin, and Ziwei Liu </div> <div class="periodical"> <em>Neural Information Processing Systems (NeurIPS),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.02350" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://alvinliu0.github.io/projects/ANGIE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/long2022fastv2v-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/long2022fastv2v-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/long2022fastv2v-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/long2022fastv2v.jpeg"> </picture> </figure> </div> <div id="long2022fastv2v" class="col-sm-9"> <div class="title">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis</div> <div class="author">Long Zhuo, Guangcong Wang, Shikai Li,  <em>Wayne Wu</em>, and Ziwei Liu </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2207.05049" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://fast-vid2vid.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jianglin2022styleganhuman-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jianglin2022styleganhuman-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jianglin2022styleganhuman-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jianglin2022styleganhuman.jpg"> </picture> </figure> </div> <div id="jianglin2022styleganhuman" class="col-sm-9"> <div class="title">StyleGAN-Human: A Data-Centric Odyssey of Human Generation</div> <div class="author">Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy,  <em>Wayne Wu</em>†, and Ziwei Liu </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIrb9hwsdcI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://stylegan-human.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xian2022semanticaware-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xian2022semanticaware-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xian2022semanticaware-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xian2022semanticaware.png"> </picture> </figure> </div> <div id="xian2022semanticaware" class="col-sm-9"> <div class="title">Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation</div> <div class="author">Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou,  <em>Wayne Wu</em>, and Bolei Zhou </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2201.07786" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://alvinliu0.github.io/projects/SSP-NeRF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/hao2022celebvhq-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/hao2022celebvhq-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/hao2022celebvhq-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/hao2022celebvhq.png"> </picture> </figure> </div> <div id="hao2022celebvhq" class="col-sm-9"> <div class="title">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</div> <div class="author">Hao Zhu*,  <em>Wayne Wu</em> *†, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2207.12393" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=Y0uxlUW4sW0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://celebv-hq.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/haoyue2022jointmodal-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/haoyue2022jointmodal-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/haoyue2022jointmodal-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/haoyue2022jointmodal.png"> </picture> </figure> </div> <div id="haoyue2022jointmodal" class="col-sm-9"> <div class="title">Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</div> <div class="author">Haoyue Cheng, Zhaoyang Liu, Hang Zhou, Chen Qian,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11573" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yuming2022text-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yuming2022text-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yuming2022text-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yuming2022text.png"> </picture> </figure> </div> <div id="yuming2022text" class="col-sm-9"> <div class="title">Text2Human: Text-Driven Controllable Human Image Generation</div> <div class="author">Yuming Jiang, Shuai Yang, Haonan Qiu,  <em>Wayne Wu</em>, Chen Change Loy, and Ziwei Liu </div> <div class="periodical"> <em>ACM Transaction on Graphics (SIGGRAPH),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=yKh4VORA_E0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yumingj.github.io/projects/Text2Human.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xinya2022eamm-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xinya2022eamm-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xinya2022eamm-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xinya2022eamm.png"> </picture> </figure> </div> <div id="xinya2022eamm" class="col-sm-9"> <div class="title">EAMM: One-Shot Emotional Talking Face via Audio-based Emotion-Aware Motion Model</div> <div class="author">Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu,  <em>Wayne Wu</em>†, Feng Xu, and Xun Cao </div> <div class="periodical"> <em>ACM Transaction on Graphics (SIGGRAPH),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15278" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yanbo2022transeditor-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yanbo2022transeditor-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yanbo2022transeditor-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yanbo2022transeditor.png"> </picture> </figure> </div> <div id="yanbo2022transeditor" class="col-sm-9"> <div class="title">TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing</div> <div class="author">Yanbo Xu, Yueqin Yin, Liming Jiang, Qianyi Wu, Chengyao Zheng, Chen Change Loy, Bo Dai, and <em>Wayne Wu</em> † </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2203.17266" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://billyxyb.github.io/TransEditor/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xian2022learning-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xian2022learning-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xian2022learning-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xian2022learning.png"> </picture> </figure> </div> <div id="xian2022learning" class="col-sm-9"> <div class="title">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</div> <div class="author">Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou,  <em>Wayne Wu</em>, Bo Dai, and Bolei Zhou </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2203.13161" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=CG632W-nIWk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://alvinliu0.github.io/projects/HA2G" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jiaqi2022progressive-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jiaqi2022progressive-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jiaqi2022progressive-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jiaqi2022progressive.png"> </picture> </figure> </div> <div id="jiaqi2022progressive" class="col-sm-9"> <div class="title">Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection</div> <div class="author">Jiaqi Tang, Zhaoyang Liu, Chen Qian,  <em>Wayne Wu</em>, and Limin Wang </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2112.04771" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/mocanet-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/mocanet-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/mocanet-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/mocanet.png"> </picture> </figure> </div> <div id="wentao2022mocanet" class="col-sm-9"> <div class="title">MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks</div> <div class="author">Wentao Zhu, Zhuoqian Yang, Ziang Di,  <em>Wayne Wu</em>†, Yizhou Wang, and Chen Change Loy </div> <div class="periodical"> <em>Association for the Advancement of Artificial Intelligence (AAAI),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2112.10082" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://yzhq97.github.io/mocanet/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/linsen2022everybody-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/linsen2022everybody-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/linsen2022everybody-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/linsen2022everybody.png"> </picture> </figure> </div> <div id="linsen2022everybody" class="col-sm-9"> <div class="title">Everybody’s Talkin’: Let Me Talk as You Want</div> <div class="author">Linsen Song,  <em>Wayne Wu</em>, Chen Qian, Ran He, and Chen Change Loy </div> <div class="periodical"> <em>Transactions on Information Forensics and Security (TIFS)</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.05201" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=tNPuAnvijQk&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/EBT/EBT.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2022deepfake-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2022deepfake-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2022deepfake-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2022deepfake.png"> </picture> </figure> </div> <div id="liming2022deepfake" class="col-sm-9"> <div class="title">DeepFakes Detection: The DeeperForensics Dataset and Challenge</div> <div class="author">Liming Jiang,  <em>Wayne Wu</em>, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>Handbook of Digital Face Manipulation and Detection, Springer,</em> 2022 </div> <div class="links"> <a href="https://link.springer.com/book/10.1007/978-3-030-87664-7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yuxin2022talking-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yuxin2022talking-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yuxin2022talking-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yuxin2022talking.png"> </picture> </figure> </div> <div id="yuxin2022talking" class="col-sm-9"> <div class="title">Talking Faces: Audio-to-Video Face Generation</div> <div class="author">Yuxin Wang, Linsen Song,  <em>Wayne Wu</em>, Chen Qian, Ran He, and Chen Change Loy </div> <div class="periodical"> <em>Handbook of Digital Face Manipulation and Detection, Springer,</em> 2022 </div> <div class="links"> <a href="https://link.springer.com/book/10.1007/978-3-030-87664-7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2021deceive-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2021deceive-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2021deceive-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2021deceive.jpeg"> </picture> </figure> </div> <div id="liming2021deceive" class="col-sm-9"> <div class="title">Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data</div> <div class="author">Liming Jiang, Bo Dai,  <em>Wayne Wu</em>, and Chen Change Loy </div> <div class="periodical"> <em>Neural Information Processing System (NeurIPS),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2111.06849" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=3Luz817WpZM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://www.mmlab-ntu.com/project/apa/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2021focal-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2021focal-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2021focal-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2021focal.jpeg"> </picture> </figure> </div> <div id="liming2021focal" class="col-sm-9"> <div class="title">Focal Frequency Loss for Image Reconstruction and Synthesis</div> <div class="author">Liming Jiang, Bo Dai,  <em>Wayne Wu</em>, and Chen Change Loy </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2012.12821" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.mmlab-ntu.com/project/ffl/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhaoyang2021tam-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhaoyang2021tam-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhaoyang2021tam-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhaoyang2021tam.png"> </picture> </figure> </div> <div id="zhaoyang2021tam" class="col-sm-9"> <div class="title">TAM: Temporal Adaptive Module for Video Recognition</div> <div class="author">Zhaoyang Liu, Limin Wang,  <em>Wayne Wu</em>, Chen Qian, and Tong Lu </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2005.06803" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/liu-zhy/temporal-adaptive-module" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/linsen2021everything-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/linsen2021everything-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/linsen2021everything-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/linsen2021everything.png"> </picture> </figure> </div> <div id="linsen2021everything" class="col-sm-9"> <div class="title">Everything’s Talkin’: Pareidolia Face Reenactment</div> <div class="author">Linsen Song*,  <em>Wayne Wu</em> *, Chaoyou Fu, Chen Qian, Chen Change Loy, and Ran He </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.05201" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=yKh4VORA_E0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/ETT/ETT.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xinya2021evp-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xinya2021evp-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xinya2021evp-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xinya2021evp.png"> </picture> </figure> </div> <div id="xinya2021evp" class="col-sm-9"> <div class="title">Audio-Driven Emotional Video Portraits</div> <div class="author">Xinya Ji, Hang Zhou, Kaisiyuan Wang,  <em>Wayne Wu</em>†, Chen Change Loy, Xun Cao, and Feng Xu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2104.07452" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=o6LQfLkizbw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://jixinya.github.io/projects/evp/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/hang2022pose-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/hang2022pose-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/hang2022pose-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/hang2022pose.png"> </picture> </figure> </div> <div id="hang2022pose" class="col-sm-9"> <div class="title">Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation</div> <div class="author">Hang Zhou, Yasheng Sun,  <em>Wayne Wu</em>, Chen Change Loy, Xiaogang Wang, and Ziwei Liu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=lNQQHIggnUg&amp;feature=emb_logo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://hangz-nju-cuhk.github.io/projects/PC-AVS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/hao2020aot-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/hao2020aot-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/hao2020aot-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/hao2020aot.png"> </picture> </figure> </div> <div id="hao2020aot" class="col-sm-9"> <div class="title">AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection</div> <div class="author">Hao Zhu, Chaoyou Fu, Qianyi Wu,  <em>Wayne Wu</em>, Chen Qian, and Ran He </div> <div class="periodical"> <em>Neural Information Processing System (NeurIPS),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2011.02674" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/xiaokang2020bidirectional-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/xiaokang2020bidirectional-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/xiaokang2020bidirectional-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/xiaokang2020bidirectional.png"> </picture> </figure> </div> <div id="xiaokang2020bidirectional" class="col-sm-9"> <div class="title">Bi-directional Cross-Modality Feature Propagation with SA Gate for RGB-D Semantic Segmentation</div> <div class="author">Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang,  <em>Wayne Wu</em>, Chen Qian, Hongsheng Li, and Gang Zeng </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2007.09183" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/charlesCXK/RGBD_Semantic_Segmentation_PyTorch" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/kaisiyuan2020mead-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/kaisiyuan2020mead-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/kaisiyuan2020mead-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/kaisiyuan2020mead.png"> </picture> </figure> </div> <div id="kaisiyuan2020mead" class="col-sm-9"> <div class="title">MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation</div> <div class="author">Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang,  <em>Wayne Wu</em>†, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2020 </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">YouTube</a> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://wywu.github.io/projects/MEAD/MEAD.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/liming2020deeperforensics-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/liming2020deeperforensics-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/liming2020deeperforensics-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/liming2020deeperforensics.png"> </picture> </figure> </div> <div id="liming2020deeperforensics" class="col-sm-9"> <div class="title">DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection</div> <div class="author">Liming Jiang, Ren Li,  <em>Wayne Wu</em>, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.03024" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=b6iKqkJht38&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://liming-jiang.com/projects/DrF1/DrF1.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhuoqian2020transmomo-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhuoqian2020transmomo-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhuoqian2020transmomo-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhuoqian2020transmomo.png"> </picture> </figure> </div> <div id="zhuoqian2020transmomo" class="col-sm-9"> <div class="title">TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting</div> <div class="author">Zhuoqian Yang*, Wentao Zhu*,  <em>Wayne Wu</em> *, Chen Qian, Qiang Zhou, Bolei Zhou, and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2003.14401" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=akbRtnRMkMk&amp;feature=youtu.be" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yzhq97.github.io/transmomo/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2019transgaga-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2019transgaga-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2019transgaga-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2019transgaga.png"> </picture> </figure> </div> <div id="wayne2019transgaga" class="col-sm-9"> <div class="title">TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</div> <div class="author"> <em>Wayne Wu</em>, Kaidi Cao, Cheng Li, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1904.09571" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="" class="btn btn-sm z-depth-0" role="button">YouTube</a> <a href="https://wywu.github.io/projects/TGaGa/TGaGa.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/keqiang2019fab-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/keqiang2019fab-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/keqiang2019fab-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/keqiang2019fab.png"> </picture> </figure> </div> <div id="keqiang2019fab" class="col-sm-9"> <div class="title">FAB: A Robust Facial Landmark Detection Framework for Motion-Blurred Videos</div> <div class="author">Keqiang Sun,  <em>Wayne Wu</em>, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou, Zuochang Ye, and Chen Qian </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1910.12100" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/KeqiangSun/FAB" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/shengju2019aggregation-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/shengju2019aggregation-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/shengju2019aggregation-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/shengju2019aggregation.png"> </picture> </figure> </div> <div id="shengju2019aggregation" class="col-sm-9"> <div class="title">Aggregation via Separation: Boosting Facial Landmark Detector with Self-Supervised Style Transition</div> <div class="author">Shengju Qian, Keqiang Sun,  <em>Wayne Wu</em>, Chen Qian, and Jiaya Jia </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1908.06440" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/shengju2019make-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/shengju2019make-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/shengju2019make-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/shengju2019make.png"> </picture> </figure> </div> <div id="shengju2019make" class="col-sm-9"> <div class="title">Make a Face: Towards Arbitrary High Fidelity Face Manipulation</div> <div class="author">Shengju Qian, Kwan-Yee Lin,  <em>Wayne Wu</em>, Yangxiaokang Liu, Quan Wang, Fumin Shen, Chen Qian, and Ran He </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1908.07191" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2018reenactgan-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2018reenactgan-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2018reenactgan-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2018reenactgan.png"> </picture> </figure> </div> <div id="wayne2018reenactgan" class="col-sm-9"> <div class="title">ReenactGAN: Learning to Reenact Faces via Boundary Transfer</div> <div class="author"> <em>Wayne Wu</em>, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1807.11079" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=LBAfeKrHMys" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2018look-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2018look-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2018look-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2018look.png"> </picture> </figure> </div> <div id="wayne2018look" class="col-sm-9"> <div class="title">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</div> <div class="author"> <em>Wayne Wu</em>, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1805.10483" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=B5IIOYlL4w0&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/LAB/LAB.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Wayne Wu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: June 14, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4GRHT7MFBP"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4GRHT7MFBP");</script> </body> </html>