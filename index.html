<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="googlea477729bc8866235"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Wayne (Wen-Yan) Wu</title> <meta name="author" content="Wayne (Wen-Yan) Wu"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/milky-way.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wayne-wu-53.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%77%75%77%65%6E%79%61%6E%30%35%30%33@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=uWfZKz4AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/wywu" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://twitter.com/wayne_wu_0503" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publication/">Publication</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/project/">Project</a> </li> <li class="nav-item "> <a class="nav-link" href="/dataset/">Dataset</a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">Software</a> </li> <li class="nav-item "> <a class="nav-link" href="/Others/">Others</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Wayne (Wen-Yan) Wu </h1> <p class="desc"></p> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/wayne_wu-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/wayne_wu-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/wayne_wu-1400.webp"></source> <img class="img-fluid z-dept-100 rounded" src="/assets/img/wayne_wu.jpg" alt="wayne_wu.jpg"> </picture> </figure> </div> <div class="clearfix"> <p>I am an incoming Research Scientist at University of California, Los Angeles, working with <a href="https://boleizhou.github.io/" target="_blank" rel="noopener noreferrer">Bolei Zhou</a>. Currently, I am a Research Scientist at Shanghai AI Laboratory, working with <a href="http://dahua.site/" target="_blank" rel="noopener noreferrer">Dahua Lin</a>. Since 2019, I have been a Cooperative Researcher at <a href="https://www.mmlab-ntu.com/" target="_blank" rel="noopener noreferrer">MMLab@NTU</a> at Nanyang Technological University, Singapore, working with <a href="https://www.mmlab-ntu.com/person/ccloy/" target="_blank" rel="noopener noreferrer">Chen Change Loy</a>. Previously, I was a director of research and development at SenseTime Group Inc., working with <a href="http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank" rel="noopener noreferrer">Xiaogang Wang</a> and <a href="http://scholar.google.com/citations?user=AerkT0YAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer">Chen Qian</a>. In June 2022, I obtained my PhD in the Department of Computer Science and Technology at Tsinghua University. My research interests lie at the intersection of Computer Vision and Graphics, with focus on <em>neural rendering, generative models and benchmarks for humans</em>.</p> <p>** <strong>Job positions are open for researchers &amp; interns at Shanghai AI Lab!</strong> **</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun, 2023</th> <td> <a href="https://omniobject3d.github.io/" target="_blank" rel="noopener noreferrer">OmniObject3D</a> is selected as Best Paper Candidate at CVPR 2023. </td> </tr> <tr> <th scope="row">Oct, 2022</th> <td> We started <a href="https://openxdlab.org.cn/home" target="_blank" rel="noopener noreferrer">OpenXDLab</a> â€“ a new large-scale 3D dataset open-source platform! ðŸ”¥ </td> </tr> <tr> <th scope="row">Sep, 2022</th> <td> We released <a href="https://github.com/openxrlab/xrnerf" target="_blank" rel="noopener noreferrer">XRNeRF</a> â€“ OpenXRLabâ€™s Neural Radiance Fields (NeRF) Toolbox! ðŸ”¥ </td> </tr> <tr> <th scope="row">Aug, 2020</th> <td> We are organizing <a href="https://competitions.codalab.org/competitions/25228" target="_blank" rel="noopener noreferrer">DeeperForensics Challenge on Real-World Face Forgery Detection</a>, ECCV 2020. </td> </tr> <tr> <th scope="row">Aug, 2020</th> <td> We are organizing <a href="https://sense-human.github.io/" target="_blank" rel="noopener noreferrer">Workshop on Sensing, Understanding and Synthesizing Humans</a>, ECCV 2020. </td> </tr> <tr> <th scope="row">Jul, 2020</th> <td> We released <a href="https://github.com/open-mmlab/mmaction2" target="_blank" rel="noopener noreferrer">MMAction2</a>, OpenMMLabâ€™s Next Generation Action Understanding Toolbox. </td> </tr> <tr> <th scope="row">Jul, 2020</th> <td> We released <a href="https://github.com/open-mmlab/mmediting" target="_blank" rel="noopener noreferrer">MMEditing</a>, OpenMMLabâ€™s Image and Video Editing Toolbox. </td> </tr> <tr> <th scope="row">Oct, 2019</th> <td> We are organizing <a href="https://openaccess.thecvf.com/ICCV2019_workshops/ICCV2019_SDL-CV" target="_blank" rel="noopener noreferrer">Workshop on Statistical Deep Learning for Computer Vision</a>, ICCV 2019. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/contributors2023renderme360-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/contributors2023renderme360-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/contributors2023renderme360-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/contributors2023renderme360.png"> </picture> </figure> </div> <div id="contributors2023renderme360" class="col-sm-9"> <div class="title">RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</div> <div class="author"> Contributors to RenderMe-360 </div> <div class="periodical"> <em>coming soon,</em> 2023 </div> <div class="links"> <a href="https://www.youtube.com/watch?v=nIgrtQwkrdg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://renderme-360.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/contributors2023dnarendering-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/contributors2023dnarendering-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/contributors2023dnarendering-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/contributors2023dnarendering.png"> </picture> </figure> </div> <div id="contributors2023dnarendering" class="col-sm-9"> <div class="title">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</div> <div class="author"> Contributors to DNA-Rendering </div> <div class="periodical"> <em>coming soon,</em> 2023 </div> <div class="links"> <a href="" class="btn btn-sm z-depth-0" role="button">YouTube</a> <a href="" class="btn btn-sm z-depth-0" role="button">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhitao2023synbody-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhitao2023synbody-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhitao2023synbody-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhitao2023synbody.png"> </picture> </figure> </div> <div id="zhitao2023synbody" class="col-sm-9"> <div class="title">SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling</div> <div class="author">Zhitao Yang,Â Zhongang Cai,Â Haiyi Mei,Â Shuai Liu,Â Zhaoxi Chen,Â Weiye Xiao,Â Yukun Wei,Â Zhongfei Qing,Â Chen Wei,Â Bo Dai,Â  <em>Wayne Wu</em>,Â Chen Qian,Â Dahua Lin,Â Ziwei Liu,Â and Lei Yang </div> <div class="periodical"> <em>Technical report, arXiv:2303.17368,</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2303.17368" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=ogXpRB9zR9A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://maoxie.github.io/SynBody/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/tong2023omniobject3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/tong2023omniobject3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/tong2023omniobject3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/tong2023omniobject3d.png"> </picture> </figure> </div> <div id="tong2023omniobject3d" class="col-sm-9"> <div class="title">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation</div> <div class="author">Tong Wu,Â Jiarui Zhang,Â Xiao Fu,Â Yuxin Wang,Â Jiawei Ren,Â Liang Pan,Â  <em>Wayne Wu</em>,Â Lei Yang,Â Jiaqi Wang,Â Chen Qian,Â Dahua Lin,Â and Ziwei Liu </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2023 </div> <div class="links"> <a href="http://arxiv.org/abs/2301.07525" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://omniobject3d.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wei2022gnr-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wei2022gnr-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wei2022gnr-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wei2022gnr.png"> </picture> </figure> </div> <div id="wei2022gnr" class="col-sm-9"> <div class="title">Generalizable Neural Performer: Learning Robust Radiance Fields for Human Novel View Synthesis</div> <div class="author">Wei Cheng,Â Su Xu,Â Jingtan Piao,Â Chen Qian,Â  <em>Wayne Wu</em>,Â Kwan-Yee Lin,Â and Hongsheng Li </div> <div class="periodical"> <em>Technical report, arXiv:2204.11798,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2204.11798" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=2COR4u1ZIuk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://generalizable-neural-performer.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhuoqian2022hg3d-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhuoqian2022hg3d-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhuoqian2022hg3d-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhuoqian2022hg3d.png"> </picture> </figure> </div> <div id="zhuoqian2022hg3d" class="col-sm-9"> <div class="title">3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation</div> <div class="author">Zhuoqian Yang,Â Shikai Li,Â  <em>Wayne Wu</em>â€ ,Â and Bo Dai </div> <div class="periodical"> <em>Technical report, arXiv:2210.06551,</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2212.07378" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=fYgVYGMLpnw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://3dhumangan.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/jianglin2022styleganhuman-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/jianglin2022styleganhuman-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/jianglin2022styleganhuman-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/jianglin2022styleganhuman.jpg"> </picture> </figure> </div> <div id="jianglin2022styleganhuman" class="col-sm-9"> <div class="title">StyleGAN-Human: A Data-Centric Odyssey of Human Generation</div> <div class="author">Jianglin Fu,Â Shikai Li,Â Yuming Jiang,Â Kwan-Yee Lin,Â Chen Qian,Â Chen Change Loy,Â  <em>Wayne Wu</em>â€ ,Â and Ziwei Liu </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=nIrb9hwsdcI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://stylegan-human.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/yuming2022text-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/yuming2022text-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/yuming2022text-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/yuming2022text.png"> </picture> </figure> </div> <div id="yuming2022text" class="col-sm-9"> <div class="title">Text2Human: Text-Driven Controllable Human Image Generation</div> <div class="author">Yuming Jiang,Â Shuai Yang,Â Haonan Qiu,Â  <em>Wayne Wu</em>,Â Chen Change Loy,Â and Ziwei Liu </div> <div class="periodical"> <em>ACM Transaction on Graphics (SIGGRAPH),</em> 2022 </div> <div class="links"> <a href="http://arxiv.org/abs/2205.15996" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=yKh4VORA_E0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yumingj.github.io/projects/Text2Human.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/linsen2021everything-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/linsen2021everything-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/linsen2021everything-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/linsen2021everything.png"> </picture> </figure> </div> <div id="linsen2021everything" class="col-sm-9"> <div class="title">Everythingâ€™s Talkinâ€™: Pareidolia Face Reenactment</div> <div class="author">Linsen Song*,Â  <em>Wayne Wu</em> *,Â Chaoyou Fu,Â Chen Qian,Â Chen Change Loy,Â and Ran He </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2021 </div> <div class="links"> <a href="http://arxiv.org/abs/2001.05201" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=yKh4VORA_E0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/ETT/ETT.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/zhuoqian2020transmomo-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/zhuoqian2020transmomo-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/zhuoqian2020transmomo-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/zhuoqian2020transmomo.png"> </picture> </figure> </div> <div id="zhuoqian2020transmomo" class="col-sm-9"> <div class="title">TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting</div> <div class="author">Zhuoqian Yang*,Â Wentao Zhu*,Â  <em>Wayne Wu</em> *,Â Chen Qian,Â Qiang Zhou,Â Bolei Zhou,Â and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2020 </div> <div class="links"> <a href="http://arxiv.org/abs/2003.14401" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=akbRtnRMkMk&amp;feature=youtu.be" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://yzhq97.github.io/transmomo/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2019transgaga-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2019transgaga-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2019transgaga-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2019transgaga.png"> </picture> </figure> </div> <div id="wayne2019transgaga" class="col-sm-9"> <div class="title">TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</div> <div class="author"> <em>Wayne Wu</em>,Â Kaidi Cao,Â Cheng Li,Â Chen Qian,Â and Chen Change Loy </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2019 </div> <div class="links"> <a href="http://arxiv.org/abs/1904.09571" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="" class="btn btn-sm z-depth-0" role="button">YouTube</a> <a href="https://wywu.github.io/projects/TGaGa/TGaGa.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2018reenactgan-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2018reenactgan-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2018reenactgan-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2018reenactgan.png"> </picture> </figure> </div> <div id="wayne2018reenactgan" class="col-sm-9"> <div class="title">ReenactGAN: Learning to Reenact Faces via Boundary Transfer</div> <div class="author"> <em>Wayne Wu</em>,Â Yunxuan Zhang,Â Cheng Li,Â Chen Qian,Â and Chen Change Loy </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1807.11079" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=LBAfeKrHMys" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/publication/wayne2018look-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/publication/wayne2018look-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/publication/wayne2018look-1400.webp"></source> <img class="img-fluid z-dept-1 rounded" src="/assets/img/publication/wayne2018look.png"> </picture> </figure> </div> <div id="wayne2018look" class="col-sm-9"> <div class="title">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</div> <div class="author"> <em>Wayne Wu</em>,Â Chen Qian,Â Shuo Yang,Â Quan Wang,Â Yici Cai,Â and Qiang Zhou </div> <div class="periodical"> <em>Conference on Computer Vision and Pattern Recognition (CVPR),</em> 2018 </div> <div class="links"> <a href="http://arxiv.org/abs/1805.10483" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.youtube.com/watch?v=B5IIOYlL4w0&amp;feature=emb_imp_woyt" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">YouTube</a> <a href="https://wywu.github.io/projects/LAB/LAB.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2023 Wayne Wu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: June 14, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-4GRHT7MFBP"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4GRHT7MFBP");</script> </body> </html>